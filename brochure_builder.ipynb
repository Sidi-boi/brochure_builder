{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BROCHURE BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: dotenv in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: beautifulSoup4 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (4.13.3)\n",
      "Collecting openai\n",
      "  Using cached openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: python-dotenv in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from dotenv) (1.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from beautifulSoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from beautifulSoup4) (4.12.2)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.8.2-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in d:\\tech\\genai\\course\\my_projects\\brochure_builder\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached openai-1.64.0-py3-none-any.whl (472 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.8.2-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: tqdm, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.64.0 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install requests dotenv beautifulSoup4 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The api key looks good\n"
     ]
    }
   ],
   "source": [
    "# Initialize the constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "OPENAI_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if OPENAI_KEY and OPENAI_KEY.startswith(\"sk-proj-\") and len(OPENAI_KEY) > 10:\n",
    "    print(\"The api key looks good\")\n",
    "else:\n",
    "    print(\"There might be a problem with the api key\")\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which represents webpage\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        \n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        if soup.body():\n",
    "            for irrelevant in soup.body(['script', 'img', 'input', 'style']):\n",
    "                irrelevant.decompose()\n",
    "        else :   \n",
    "            self.text = ''\n",
    "        \n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "        def get_contents(self):\n",
    "            return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n",
    "        \n",
    "\n",
    "# website = Website(\"https://www.cypherock.com/?srsltid=AfmBOooDbHMbrl25kt6F4EwMY8zR_Oit2VuZr-GODuBv5KasLaBdqUn9\")\n",
    "# website.links\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a list of links found on a webpage. You are able to decide which of the links would be most relevant to include in a brochure about the company, such as links to an About page, or a Company page, or Careers/Jobs pages.\n",
      "You should respond in JSON as in this example:\n",
      "{\n",
      "    \"links\": [\n",
      "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
      "        {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
    "such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
    "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "link_system_prompt += \"\"\"\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# print(link_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "                Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt\n",
    "\n",
    "# print(get_links_user_prompt(website))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'links': [{'type': 'about page', 'url': 'https://www.cypherock.com/about'},\n",
       "  {'type': 'product page',\n",
       "   'url': 'https://www.cypherock.com/product/cypherock-x1'},\n",
       "  {'type': 'how it works page',\n",
       "   'url': 'https://www.cypherock.com/how-it-works'},\n",
       "  {'type': 'features page', 'url': 'https://www.cypherock.com/features'},\n",
       "  {'type': 'careers page', 'url': 'https://angel.co/company/cypherock-wallet'},\n",
       "  {'type': 'faq page', 'url': 'https://www.cypherock.com/faq'},\n",
       "  {'type': 'company news',\n",
       "   'url': 'https://www.entrepreneur.com/en-in/news-and-trends/cypherock-raises-1million-to-build-mpc-based-hardware/440209'}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_links(website):\n",
    "    website = Website(\"https://www.cypherock.com/?srsltid=AfmBOooDbHMbrl25kt6F4EwMY8zR_Oit2VuZr-GODuBv5KasLaBdqUn9\")\n",
    "    response = openai.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            { 'role':'system', 'content':link_system_prompt},\n",
    "            {'role':'user', 'content':get_links_user_prompt(website)}\n",
    "        ],\n",
    "        response_format={'type':'json_object'}\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    return json.loads(result)\n",
    "\n",
    "# get_links(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
